# Machine Learning Course HomeWorks 
A short discription about each question has provided below.
## HW1 - Bayes Classifier & Parametric Density Estimation

### Theoretical Section
* Q1: Bayes error and cauchy distribution
* Q2: Bayesian minimum risk classifier
* Q3: Bayes classifier decision boundary
* Q4: Bayes classifier and normal distribution
* Q5: Parameter estimation using maximum likelihood (MLE) and apply bayes classifier
* Q6:  Parameter estimation using MLE and maximum a posteriori (MAP)

### Programming Section
 * Q7: Implementing naive bayes classifier from scratch
 * Q8: Implementing a simple pixel classification

## HW2 - Non-parametric Density Esimation

### Theoretical Section
* Q1: Parzen window variance
* Q2: Parzen window mean
* Q3: Linear regression with L1/L2 regularization
* Q4: Decision boundary using nearest-neighbor rule 
* Q5: Nearest-neighbor classifier error

### Programming Section
* Q6: Implementing parzen window density estimation from scratch
* Q7: Classifying using the Parzen Window
* Q8: Implementing Logistic Regression and K-Nearest Neighbors (KNN) classifiers from scratch, and use them to classify the Wheat Seeds dataset
* Q9: Implementing Linear Regression from scratch, and use it to classify the Marketing Campaign dataset

## HW3 - Decision Tree & AdaBoost

### Theoretical Section
* Q1: AdaBoost concept
* Q2: AdaBoost classifier error
* Q5: Decision tree and information gain

### Programming Section
* Q3: classifying the Credit Scoring Sample dataset using a Random Forest classifier and a Bagging classifier. Additionally, utilizing Bootstrap sampling to estimate the mean of customers' age
* Q4: Implementing AdaBoost classifier from scratch, and use it to classify iris dataset
* Q6: Implementing Decision Tree from scratch using ID3 algorithm, and use it to classify prison dataset

## HW4 - Kernel Methods & Neural Network

### Theoretical Section
* Q1: Multi Layer Perceptron (MLP) and activation function
* Q2: Forward and backward propagation in neural networks
* Q5: Kernel methods and mean of data in transfered space

### Programming Section
* Q3: Comparing MLP and CNN with respect to translational invariance feature
* Q4: 
    - Classifying a 4-sample dataset in a 2D space with hard SVM
    - Finding a mapping to transfer a dataset to a new space where it Is linearly separable
* Q6: Classifying MNIST dataset with kernel SVM. Linear, RBF, and polynomial kernels will be tested.

## HW5 - Clustering & Expectation-Maximization

### Theoretical Section
* Q1: Calculate within-class and between-class scatter matrices
* Q2: Model selection concepts
* Q3: Expectation-Maximization (EM) method for exponential mixture model 
* Q4: Estimate Gaussian Mixture Model (GMM) using neural network 

### Programming Section
* Q5: Implementing Principal Component Analysis (PCA) from scratch, and use it to reduce dimensionality of fashion-MNIST dataset
* Q6: GMM density estimation for the MNIST dataset
* Q7: Clustering customers dataset using k-means algorithm. Also find optimal value of k using diffrent methods and score functions, such as:
    - K-means Distortion and Elbow Method
    - Silhoutte Score
    - Dacies-Bouldin Index
    - Calinski-Harabasz Index
    - Dunn Index